{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPKtFSjxtGeI5y//5IDyPR3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeondub1121/prac-colab/blob/main/Untitled13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import urllib3\n",
        "import zipfile\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "num_samples = 33000\n",
        "\n",
        "!wget -c http://www.manythings.org/anki/fra-eng.zip && unzip -o fra-eng.zip\n",
        "\n",
        "def unicode_to_ascii(s):\n",
        "  # 프랑스어 악센트(accent) 삭제\n",
        "  # 예시 : 'déjà diné' -> deja dine\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(sent):\n",
        "  # 악센트 삭제 함수 호출\n",
        "  sent = unicode_to_ascii(sent.lower())\n",
        "\n",
        "  # 단어와 구두점 사이에 공백을 만듭니다.\n",
        "  # Ex) \"he is a boy.\" => \"he is a boy .\"\n",
        "  sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
        "\n",
        "  # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환합니다.\n",
        "  sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
        "\n",
        "  # 다수 개의 공백을 하나의 공백으로 치환\n",
        "  sent = re.sub(r\"\\s+\", \" \", sent)\n",
        "  return sent\n",
        "\n",
        "def load_preprocessed_data():\n",
        "  encoder_input, decoder_input, decoder_target = [], [], []\n",
        "\n",
        "  with open(\"fra.txt\", \"r\") as lines:\n",
        "    for i, line in enumerate(lines):\n",
        "      # source 데이터와 target 데이터 분리\n",
        "      src_line, tar_line, _ = line.strip().split('\\t')\n",
        "\n",
        "      # source 데이터 전처리\n",
        "      src_line = [w for w in preprocess_sentence(src_line).split()]\n",
        "\n",
        "      # target 데이터 전처리\n",
        "      tar_line = preprocess_sentence(tar_line)\n",
        "      tar_line_in = [w for w in (\"<sos> \" + tar_line).split()]\n",
        "      tar_line_out = [w for w in (tar_line + \" <eos>\").split()]\n",
        "\n",
        "      encoder_input.append(src_line)\n",
        "      decoder_input.append(tar_line_in)\n",
        "      decoder_target.append(tar_line_out)\n",
        "\n",
        "      if i == num_samples - 1:\n",
        "        break\n",
        "\n",
        "  return encoder_input, decoder_input, decoder_target\n",
        "\n",
        "# 전처리 테스트\n",
        "en_sent = u\"Have you had dinner?\"\n",
        "fr_sent = u\"Avez-vous déjà diné?\"\n",
        "\n",
        "print('전처리 전 영어 문장 :', en_sent)\n",
        "print('전처리 후 영어 문장 :',preprocess_sentence(en_sent))\n",
        "print('전처리 전 프랑스어 문장 :', fr_sent)\n",
        "print('전처리 후 프랑스어 문장 :', preprocess_sentence(fr_sent))\n",
        "\n",
        "sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()\n",
        "print('인코더의 입력 :',sents_en_in[:5])\n",
        "print('디코더의 입력 :',sents_fra_in[:5])\n",
        "print('디코더의 레이블 :',sents_fra_out[:5])\n",
        "\n",
        "def build_vocab(sents):\n",
        "  word_list = []\n",
        "\n",
        "  for sent in sents:\n",
        "      for word in sent:\n",
        "        word_list.append(word)\n",
        "\n",
        "  # 각 단어별 등장 빈도를 계산하여 등장 빈도가 높은 순서로 정렬\n",
        "  word_counts = Counter(word_list)\n",
        "  vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "\n",
        "  word_to_index = {}\n",
        "  word_to_index['<PAD>'] = 0\n",
        "  word_to_index['<UNK>'] = 1\n",
        "\n",
        "  # 등장 빈도가 높은 단어일수록 낮은 정수를 부여\n",
        "  for index, word in enumerate(vocab) :\n",
        "    word_to_index[word] = index + 2\n",
        "\n",
        "  return word_to_index\n",
        "\n",
        "src_vocab = build_vocab(sents_en_in)\n",
        "tar_vocab = build_vocab(sents_fra_in + sents_fra_out)\n",
        "\n",
        "src_vocab_size = len(src_vocab)\n",
        "tar_vocab_size = len(tar_vocab)\n",
        "print(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))\n",
        "\n",
        "index_to_src = {v: k for k, v in src_vocab.items()}\n",
        "index_to_tar = {v: k for k, v in tar_vocab.items()}\n",
        "\n",
        "def texts_to_sequences(sents, word_to_index):\n",
        "  encoded_X_data = []\n",
        "  for sent in tqdm(sents):\n",
        "    index_sequences = []\n",
        "    for word in sent:\n",
        "      try:\n",
        "          index_sequences.append(word_to_index[word])\n",
        "      except KeyError:\n",
        "          index_sequences.append(word_to_index['<UNK>'])\n",
        "    encoded_X_data.append(index_sequences)\n",
        "  return encoded_X_data\n",
        "\n",
        "encoder_input = texts_to_sequences(sents_en_in, src_vocab)\n",
        "decoder_input = texts_to_sequences(sents_fra_in, tar_vocab)\n",
        "decoder_target = texts_to_sequences(sents_fra_out, tar_vocab)\n",
        "\n",
        "# 상위 5개의 샘플에 대해서 정수 인코딩 전, 후 문장 출력\n",
        "# 인코더 입력이므로 <sos>나 <eos>가 없음\n",
        "for i, (item1, item2) in zip(range(5), zip(sents_en_in, encoder_input)):\n",
        "    print(f\"Index: {i}, 정수 인코딩 전: {item1}, 정수 인코딩 후: {item2}\")\n",
        "\n",
        "def pad_sequences(sentences, max_len=None):\n",
        "    # 최대 길이 값이 주어지지 않을 경우 데이터 내 최대 길이로 패딩\n",
        "    if max_len is None:\n",
        "        max_len = max([len(sentence) for sentence in sentences])\n",
        "\n",
        "    features = np.zeros((len(sentences), max_len), dtype=int)\n",
        "    for index, sentence in enumerate(sentences):\n",
        "        if len(sentence) != 0:\n",
        "            features[index, :len(sentence)] = np.array(sentence)[:max_len]\n",
        "    return features\n",
        "\n",
        "encoder_input = pad_sequences(encoder_input)\n",
        "decoder_input = pad_sequences(decoder_input)\n",
        "decoder_target = pad_sequences(decoder_target)\n",
        "\n",
        "print('인코더의 입력의 크기(shape) :',encoder_input.shape)\n",
        "print('디코더의 입력의 크기(shape) :',decoder_input.shape)\n",
        "print('디코더의 레이블의 크기(shape) :',decoder_target.shape)\n",
        "\n",
        "indices = np.arange(encoder_input.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "print('랜덤 시퀀스 :',indices)\n",
        "\n",
        "encoder_input = encoder_input[indices]\n",
        "decoder_input = decoder_input[indices]\n",
        "decoder_target = decoder_target[indices]\n",
        "\n",
        "print([index_to_src[word] for word in encoder_input[30997]])\n",
        "print([index_to_tar[word] for word in decoder_input[30997]])\n",
        "print([index_to_tar[word] for word in decoder_target[30997]])\n",
        "\n",
        "n_of_val = int(33000*0.1)\n",
        "print('검증 데이터의 개수 :',n_of_val)\n",
        "\n",
        "encoder_input_train = encoder_input[:-n_of_val]\n",
        "decoder_input_train = decoder_input[:-n_of_val]\n",
        "decoder_target_train = decoder_target[:-n_of_val]\n",
        "\n",
        "encoder_input_test = encoder_input[-n_of_val:]\n",
        "decoder_input_test = decoder_input[-n_of_val:]\n",
        "decoder_target_test = decoder_target[-n_of_val:]\n",
        "\n",
        "print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n",
        "print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n",
        "print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n",
        "print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n",
        "print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n",
        "print('테스트 target 레이블의 크기 :',decoder_target_test.shape)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, src_vocab_size, embedding_dim, hidden_units):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(src_vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_units, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x.shape == (batch_size, seq_len, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "        # hidden.shape == (1, batch_size, hidden_units), cell.shape == (1, batch_size, hidden_units)\n",
        "        outputs, (hidden, cell) = self.lstm(x)\n",
        "        # 인코더의 출력은 hidden state, cell state\n",
        "        return outputs, hidden, cell\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, tar_vocab_size, embedding_dim, hidden_units):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(tar_vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim + hidden_units, hidden_units, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_units, tar_vocab_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x, encoder_outputs, hidden, cell):\n",
        "        # x.shape == (batch_size, seq_len, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # -------------------------------\n",
        "        # Dot product attention\n",
        "        # -------------------------------\n",
        "        # attention_scores.shape: (batch_size, source_seq_len, 1)\n",
        "        attention_scores = torch.bmm(\n",
        "            encoder_outputs,\n",
        "            hidden.transpose(0, 1).transpose(1, 2)\n",
        "        )\n",
        "\n",
        "        # attention_weights.shape: (batch_size, source_seq_len, 1)\n",
        "        attention_weights = self.softmax(attention_scores)\n",
        "\n",
        "        # context_vector.shape: (batch_size, 1, hidden_units)\n",
        "        context_vector = torch.bmm(\n",
        "            attention_weights.transpose(1, 2),\n",
        "            encoder_outputs\n",
        "        )\n",
        "\n",
        "        # Repeat context_vector to match seq_len\n",
        "        seq_len = x.shape[1]\n",
        "        # context_vector_repeated.shape: (batch_size, target_seq_len, hidden_units)\n",
        "        context_vector_repeated = context_vector.repeat(1, seq_len, 1)\n",
        "\n",
        "        # Concatenate context vector and embedded input\n",
        "        # x.shape: (batch_size, target_seq_len, embedding_dim + hidden_units)\n",
        "        x = torch.cat((x, context_vector_repeated), dim=2)\n",
        "\n",
        "        # 디코더의 LSTM으로 인코더의 hidden state, cell state 전달.\n",
        "        # output.shape == (batch_size, seq_len, hidden_units)\n",
        "        # hidden.shape == (1, batch_size, hidden_units)\n",
        "        # cell.shape == (1, batch_size, hidden_units)\n",
        "        output, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
        "\n",
        "        # output.shape: (batch_size, seq_len, tar_vocab_size)\n",
        "        output = self.fc(output)\n",
        "\n",
        "        # 디코더의 출력은 예측값, hidden state, cell state\n",
        "        return output, hidden, cell\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        encoder_outputs, hidden, cell = self.encoder(src)\n",
        "\n",
        "        # 훈련 중에는 디코더의 출력 중 오직 output만 사용한다.\n",
        "        output, _, _ = self.decoder(trg, encoder_outputs, hidden, cell)\n",
        "        return output\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fN6poXC_tFk1",
        "outputId": "cb679877-b8a4-497d-f2cb-f0570c316fec"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-17 07:31:31--  http://www.manythings.org/anki/fra-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n",
            "Archive:  fra-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: fra.txt                 \n",
            "전처리 전 영어 문장 : Have you had dinner?\n",
            "전처리 후 영어 문장 : have you had dinner ?\n",
            "전처리 전 프랑스어 문장 : Avez-vous déjà diné?\n",
            "전처리 후 프랑스어 문장 : avez vous deja dine ?\n",
            "인코더의 입력 : [['go', '.'], ['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.']]\n",
            "디코더의 입력 : [['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'en', 'route', '!'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!']]\n",
            "디코더의 레이블 : [['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['en', 'route', '!', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>']]\n",
            "영어 단어 집합의 크기 : 4508, 프랑스어 단어 집합의 크기 : 7889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 33000/33000 [00:00<00:00, 1266755.43it/s]\n",
            "100%|██████████| 33000/33000 [00:00<00:00, 230849.85it/s]\n",
            "100%|██████████| 33000/33000 [00:00<00:00, 1175983.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index: 0, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [27, 2]\n",
            "Index: 1, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [27, 2]\n",
            "Index: 2, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [27, 2]\n",
            "Index: 3, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [27, 2]\n",
            "Index: 4, 정수 인코딩 전: ['hi', '.'], 정수 인코딩 후: [736, 2]\n",
            "인코더의 입력의 크기(shape) : (33000, 7)\n",
            "디코더의 입력의 크기(shape) : (33000, 16)\n",
            "디코더의 레이블의 크기(shape) : (33000, 16)\n",
            "랜덤 시퀀스 : [15566  2024 21935 ...  5814 14863 18513]\n",
            "['she', 'can', 'handle', 'it', '.', '<PAD>', '<PAD>']\n",
            "['<sos>', 'elle', 'peut', 's', 'en', 'sortir', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "['elle', 'peut', 's', 'en', 'sortir', '.', '<eos>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "검증 데이터의 개수 : 3300\n",
            "훈련 source 데이터의 크기 : (29700, 7)\n",
            "훈련 target 데이터의 크기 : (29700, 16)\n",
            "훈련 target 레이블의 크기 : (29700, 16)\n",
            "테스트 source 데이터의 크기 : (3300, 7)\n",
            "테스트 target 데이터의 크기 : (3300, 16)\n",
            "테스트 target 레이블의 크기 : (3300, 16)\n"
          ]
        }
      ]
    }
  ]
}